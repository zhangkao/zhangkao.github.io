
<!-- saved from url=(0026)http://zhangkao.github.io/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	
	<title>Kao Zhang's home page</title>
	<meta content="Zhang Kao, zhangkao.github.io" name="keywords">
	<style media="screen" type="text/css">html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td {
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}
a {
  color: #1772d0;
  text-decoration:none;
}
a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
}
a.paper {
  font-weight: bold;
  font-size: 12pt;
}
b.paper {
  font-weight: bold;
  font-size: 12pt;
}
* {
  margin: 0pt;
  padding: 0pt;
}
body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 800px;
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 14px;
  background: #eee;
}
h2 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15pt;
  font-weight: 700;
}
h3 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  font-weight: 700;
}
strong {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 13px;
  font-weight:bold;
}
ul { 
  list-style: circle;
}
img {
  border: none;
}
li {
  padding-bottom: 0.5em;
  margin-left: 1.4em;
}
alert {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 13px;
  font-weight: bold;
  color: #FF0000;
}
em, i {
	font-style:italic;
}
div.section {
  clear: both;
  margin-bottom: 1.5em;
  background: #eee;
}
div.spanner {
  clear: both;
}
div.paper {
  clear: both;
  margin-top: 0.5em;
  margin-bottom: 1em;
  border: 1px solid #ddd;
  background: #fff;
  padding: 1em 1em 1em 1em;
}
div.paper div {
  padding-left: 230px;
}
img.paper {
  margin-bottom: 0.5em;
  float: left;
  width: 200px;
}
span.blurb {
  font-style:italic;
  display:block;
  margin-top:0.75em;
  margin-bottom:0.5em;
}
pre, code {
  font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
  margin: 1em 0;
  padding: 0;
}
div.paper pre {
  font-size: 0.9em;
}
</style>

<link href="./zhangkao_files/css" rel="stylesheet" type="text/css"><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz' rel='stylesheet' type='text/css'>-->
<script async="" src="./zhangkao_files/analytics.js"></script><script async="" src="./zhangkao_files/analytics.js"></script><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');



</script><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-66888300-1', 'auto');
  ga('send', 'pageview');

</script><script type="text/javascript" src="./zhangkao_files/jquery-1.12.4.min.js"></script></head>


<body>
<div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 140px;">
<div style="margin: 0px auto; width: 100%;">
<img title="Zhang Kao" style="float: left; padding-left: .01em; height: 140px;" src="./zhangkao_files/kaozhang_whu1.png">
<div style="padding-left: 12em; vertical-align: top; height: 120px;"><span style="line-height: 150%; font-size: 20pt;">Kao Zhang (<a href="https://faculty.nuist.edu.cn/zhangkao/zh_CN/index.htm">张考</a>)</span><br>
<!--<span><strong>Postdoc research fellow</strong></span><br>-->
<!--<span>Lab. of Intelligent Information Processing<br>-->
<span>School of Artificial Intelligence/School of Future Technology (人工智能学院)</span><br>
<span>Nanjing University of Information Science and Technology (南京信息工程大学)</span><br>
<span><strong>Address</strong>: Room A1911, Linjiang Building, No.219, Ningliu Road, Nanjing, China</span><br>
<span><strong>Email  </strong>: kaozhang@nuist.edu.cn; zhangkao@whu.edu.cn </span> <br> 
<span><strong>GitHub  </strong>: <a href="https://github.com/zhangkao">https://github.com/zhangkao</a></span> <br> 
</span></div>
</div>
</div>
<!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->

<div style="clear: both;">
<div class="section">
	<h2>About Me 
	<a href="https://faculty.nuist.edu.cn/zhangkao/zh_CN/index.htm">(中文) </a>
	<a href="http://zhangkao.github.io/zhangkao_files/cv_en_zhangkao.pdf">(CV) </a>
	<a href="http://zhangkao.github.io/zhangkao_files/CV1-20230315.pdf">(CV_ZH) </a>
	<a href="https://scholar.google.com/citations?user=UsK-rXQAAAAJ&hl=zh-CN">(Google scholar) </a>
	</h2>
	<div class="paper">
	Kao Zhang received his Ph.D at <a href="http://iip.whu.edu.cn/">Lab. of Intelligent Information Processing (IIP)</a> from Wuhan University, Wuhan, China, in 2020, under the supervision of <a href="https://zhenzhong-chen.github.io/">Prof.Zhenzhong Chen</a>. Formerly, he finished the B.Eng. and M.Eng degrees at <a href="http://cvrs.whu.edu.cn/">Computer Vision &amp; Remote Sensing Lab (CVRS)</a> in 2014 and 2016 respectively, under the guidance of Prof.Jian Yao. He was a postdoctoral fellow at Wuhan University, working on visual saliency pridection, a researcher at Tencent, Shenzhen, working on video processing, and a visiting student at <a href="http://www-percept.irisa.fr/">PERCEPT team </a> of INRIA, Rennes, France working on UAV video saliency prediction. His current research interests include visual attention, image/video processing, remote sensing and metaverse.<br><br>
	
	<font color="#FF0000">We are looking for self-motivated undergraduate/graduate students. If you are interested in joining us, please feel free to contact me with your CV!
	
	[2025级硕士研究生名额若干！也欢迎感兴趣的本科生加入!]</font><br><br>
	
	<!--Main research fields: <br>
	<ul>
    <li> Visual saliency modeling</li>
    <li> Object detection and tracking in satellite/UAV videos</li>
    <li> Multimodal Emotion recognition</li>
	</ul>-->
	</div>
</div>
</div>


<div style="clear: both;">
<div class="section">
  <h2>News</h2>
  <div class="paper">
    <ul>
		<li> 2024.05: One paper is accepted by Neurocomputing.</li>
		<li> 2024.02: One paper is accepted by JVCI.</li>
		<li> 2023.10: One paper is accepted by JVCI.</li>
		<li> 2022.10: One paper is accepted by ISPRS JPRS.</li>
		<li> 2022.10: One paper is accepted by IEEE VICP.</li>
		<li> 2022.09: Supported by NSFC.</li>
		<li> 2021.08: Supported by Postdoctoral Innovative Research Position Funding of Hubei Province of China.</li>
		<li> 2021.06: Supported by China Postdoctoral Science Foundation.</li>
		<li> 2020.10: One paper is accepted by IEEE TIP.</li>
    </ul>
  </div>
</div>
</div>


<div style="clear: both;">
<div class="section">
  <h2>Research Interest</h2>
  <div class="paper">
    <ul>
	
	<li> Visual attention: Video/Image/RGBD/VR/UAV saliency prediction.</li>  
	<!--<li> Image/Video Processing: Image/Video reframing, denoising, super-resolution.</li> -->
    <li> Remote sensing video analysis: Object detection, tracking and recognition in satellite/UAV videos. </li>
	<li> Metaverse: Multimodal (text, image, video, and sound) Emotion analysis, Virtual Reality technology. </li>
    </ul>
  </div>
</div>
</div>


<div style="clear: both;">
<div class="section">
  <h2>Education and Experience</h2>
  <div class="paper">
    <ul>
	<li> 2010.09-2014.06, School of Remote Sensing and Information Engineering, WHU, B.E.</li>  
	<li> 2014.09-2016.06, School of Remote Sensing and Information Engineering, WHU, M.E.</li> 
    <li> 2016.09-2020.06, School of Remote Sensing and Information Engineering, WHU, Ph.D. </li>	
	<li> 2020.07-2020.12, Intelligent Media Team, Media Lab, CSIG, Tencent, Visiting Researcher. </li>	
	<li> 2020.12-2023.02, School of Remote Sensing and Information Engineering, WHU, Postdoc. </li>	
	<li> 2023.03- Now&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;, School of Artificial Intelligence, NUIST, Lecturer. </li>	
    </ul>
  </div>
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>Selected Publications</h2>
  <div class="paper">

  <h3 id="confpapers">Journals</h3>
  <ul>
  
  	<li><strong>Kao Zhang</strong>, Zhenzhong Chen, Songnan Li, Shan Liu. <i>An Efficient Saliency Prediction Model for Unmanned Aerial Vehicle Video</i>. ISPRS Journal of Photogrammetry and Remote Sensing, vol. 194, pp. 152-166, 2022.
	<a href="https://www.sciencedirect.com/science/article/pii/S0924271622002763">[PDF]</a> 
	<a href="https://github.com/zhangkao/IIP_UAVSal_Saliency">[Code]</a>
	</li>
	
	
  	
	<li><strong>Kao Zhang</strong>, Zhenzhong Chen, Shan Liu. <i>A Spatial-Temporal Recurrent Neural Network for Video Saliency Prediction.</i> IEEE Transactions on Image Processing (TIP), vol. 30, pp. 572-587, 2021.
	<a href="https://ieeexplore.ieee.org/document/9263359">[PDF]</a> 
	<a href="https://github.com/zhangkao/IIP_STRNN_Saliency">[Code]</a>
	</li>
	
	
  <li><strong>Kao Zhang</strong>, Zhenzhong Chen. <i>Video Saliency Prediction Based on Spatial-Temporal Two-Stream Network.</i> IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), vol. 29, no. 12, pp. 3544-3557, 2019. 
	<a href="https://ieeexplore.ieee.org/document/8543830">[PDF]</a> 
	<a href="https://github.com/zhangkao/IIP_TwoS_Saliency">[Code]</a>
	</li>


  <li>Di Liu, <strong>Kao Zhang</strong>, Zhenzhong Chen. <i>Attentive Cross-Modal Fusion Network for RGB-D Saliency Detection.</i> IEEE Transactions on Multimedia (TMM), vol. 23, pp. 967-981, 2021.
	<a href="https://ieeexplore.ieee.org/document/9082817">[PDF]</a>
	</li>


  <li>Hao Cai*, <strong>Kao Zhang*</strong>, Zhao Chen, Chenxi Jiang, Zhenzhong Chen. <i>Video saliency prediction for first-person view UAV videos: Dataset and benchmark</i>. Neurocomputing, 2024: 127876. (co-first author)
	<a href="https://www.sciencedirect.com/science/article/pii/S0925231224006477">[PDF]</a>
	</li>

  <li>Zhao Chen*, <strong>Kao Zhang*</strong>, Hao Cai, Xiaoying Ding, Chenxi Jiang, Zhenzhong Chen. <i>Audio-visual saliency prediction for movie viewing in immersive environments: Dataset and benchmarks</i>. Journal of Visual Communication and Image Representation (JVCI), 2024:104095. (co-first author)
    <a href="https://www.sciencedirect.com/science/article/pii/S1047320324000506">[PDF]</a>
  </li>

  <li>Yang Li*, <strong>Kao Zhang*</strong>, Zhao Chen, Wanping Ouyang, Mingpeng Cui, Chenxi Jiang, Daiqin Yang and Zhenzhong Chen. <i>Towards Object Tracking for Quadruped Robots</i>. Journal of Visual Communication and Image Representation (JVCI), 2023, 97: 103958. (co-first author)
	<a href="https://www.sciencedirect.com/science/article/pii/S1047320323002080">[PDF]</a>
	</li>
	
	<li>Jing Ling, <strong>Kao Zhang</strong>, Yingxue Zhang, Daiqin Yang, Zhenzhong Chen. <i>A saliency prediction model on 360 degree images using color dictionary based sparse representation.</i> Signal Processing: Image Communication (SPIC), vol. 69, pp. 60-68, 2017. 
	</li>
	
	<li>Zhaopeng Hu, Daiqin Yang, <strong>Kao Zhang</strong>, Zhenzhong Chen. <i>Object Tracking in Satellite Videos Based on Convolutional Regression Network with Appearance and Motion Features.</i> IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing (JSTARS), vol. 13, no. 12, pp. 783-793, 2020.
	</li>  
	
  </ul>
	
  <h3 id="confpapers">Conferences</h3>
  <ul>
  
	<li><strong>Kao Zhang</strong>, Yan Shang, Songnan Li, Shan Liu, Zhenzhong Chen. <i>SalCrop: Spatio-temporal Saliency Based Video Cropping.</i> in Proc. IEEE International Conference on Visual Communications and Image Processing (VCIP), 2022, (Demo, Oral, Poster).
	</li>  
	
	
	<li>Di Liu, Yaosi Hu, <strong>Kao Zhang</strong>, Zhenzhong Chen. <i>Two-stream refinement network for RGB-D saliency detection.</i> in Proc. IEEE International Conference on Image Processing (ICIP), 2019, pp. 3925-3929.
	</li>  
	
	<li>Xixi Li, Di Liu, <strong>Kao Zhang</strong>, Zhenzhong Chen. <i>Layout-Driven Top-Down Saliency Detection for Webpage.</i> in Proc. Pacific Rim Conference on Multimedia (PCM), 2017. 438-446. 
	</li>  
	  
	<li>Ruiqian Zhang, Jian Yao, <strong>Kao Zhang</strong>, Chen Feng and Jiadong Zhang. <i>S-CNN-Based Ship Detection from High-Resolution Remote Sensing Images.</i> in Proc. ISPRS-Int. Arch. Photogram. Remote Sens. Spatial Inf. Sci. (ISPRS), 2016, pp. 423-430. (<strong>Best Poster Award</strong>).
        </li> 
	
	
	<li>Yuan Liu, <strong>Kao Zhang</strong>, Jian Yao, Tong He, Yahui Liu, and Jinge Tu. <i>An Efficient Method for Text Detection from Indoor Panorama Images Using Extremal Regions.</i> in Proc. IEEE International Conference on Information and Automation (ICIA), 2015, (Oral).
    	</li>
        
	
	<li>Tong He, Jian Yao, <strong>Kao Zhang</strong>, Yaolin Hou, Shiyao Han. <i>Accurate Multi-Scale License Plate Localization Via Image Saliency.</i> in Proc. IEEE Conference on Intelligent Transportation Systems (ITSC), 2014, pp. 1567-1572, (Oral).
        <!--<a href="http://zhangkao.github.io/zhangkao_files/2014-ITSC-VLPL.pdf">[PDF1]</a> 
	<a href="http://zhangkao.github.io/zhangkao_files/B.S.thesis.pdf">[PDF2]</a> -->
	

  </ul>	
  
  <h3 id="confpapers">Patent</h3>
  <ul>
    <li>Zhenzhong Chen, Zhao Chen, <strong>Kao Zhang</strong>. <i>A video saliency prediction method and system based on audio and video features.</i>  CN202310247030.1, 2023.
    </li>

    <li>Zhenzhong Chen, Yang Li, <strong>Kao Zhang</strong>. <i>An object tracking method for quadruped robots based on siamese network.</i>  CN202310399358.5, 2023.
    </li>

    <li><strong>Kao Zhang</strong>, Songnan Li. <i>Image cropping methods, devices, computer equipment and storage media.</i>  CN202011644040.1, 2021.
    </li>

    <li>Jian Yao, <strong>Kao Zhang</strong>, Tong He, and Sa Zhu. <i>Accurate Multi-Scale License Plate Localization Based on Affine Rectification.</i> CN201410077985.8, 2014.
    </li>

  </ul>    
  
  <h3 id="confpapers">Software Copyright</h3>
  <ul>
	<li>Jian Yao, <strong>Kao Zhang</strong>, Tong He, et al. <i>Panorama Post-Processing Software.</i> 2015R11S199708, 2015.
	</li>

  </ul>   
  
  </div>
</div>
</div>


<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Selected Awards</h2>
<div class="paper">
    <ul>
	
	<li> 2021, Second-Class Prize of Graduate Academic Innovation Award, Wuhan University. </li>
	
	<li> 2018, Grand Winner Prize on Images in ICME2018 Grand Challenge (GC) – Salient360!.
	<!--<a href="http://zhangkao.github.io/zhangkao_files/Salient360-IMG-Winner-2018.png">[PNG]</a>
	<a href="https://github.com/zhangkao/IIP_Salient360_2018">[Code]</a>-->
	</li> 
	
	<li> 2018, 1st place on track: Prediction of Head Saliency for Images in ICME2018 GC–Salient360!.
	<!--<a href="http://zhangkao.github.io/zhangkao_files/Salient360-IMG-H-2018.png">[PNG]</a>-->
	</li> 
	
	<li> 2018, 1st place on track: Prediction of Head+Eye Saliency for Videos in ICME2018 GC–Salient360!.
	<!--<a href="http://zhangkao.github.io/zhangkao_files/Salient360-VID-HE-2018.png">[PNG]</a>-->
	</li> 
		
	<li> 2017, Best Head Movement Prediction Student Prize in ICME2017 GC–Salient360!. 
	<!--<a href="http://zhangkao.github.io/zhangkao_files/Salient360-IMG-H-2017.jpg">[JPG]</a>
	<a href="https://github.com/zhangkao/IIP_Salient360_2017">[Code]</a>-->
	</li>
	
	<!--<li> 2016, Best Poster Award on ISPRS Congresses.
	<a href="http://zhangkao.github.io/zhangkao_files/2016-Best-poster.pdf">[PDF]</a>-->
	</li>
	
	<li> 2014, Second-Class Prize of the National Graduate Contest on Smart-City Technology and Creative Design, Video Challenge--Face Detection Section.
	<!--<a href="http://zhangkao.github.io/zhangkao_files/SmartCity2014.jpg">[JPG]</a>-->
	</li>

	
	<li> 2014, Excellent Bachelor’s Degree Thesis of Hubei Province. 
	<!--<a href="http://zhangkao.github.io/zhangkao_files/B.S.Thesis_prize.jpg.jpg">[JPG]</a>-->
	</li>
	<li> 2014-2016, Excellent Graduate Students of Wuhan University. </li>
    <li> 2010-2012, Excellent Undergraduate Students of Wuhan University. </li>
	<li> 2014-2016, First Class Scholarship of Wuhan University. </li>

	
    </ul>
</div>
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>Funding</h2>
  <div class="paper">
    <ul>
		<li> 2024, 南京信息工程大学人才科研启动项目, 多源无人机视频显著性检测, 主持, 2024.1-2026.10 </li>	
		<li> 2023, 国家社会科学基金青年项目, 增强网络意识形态风险防范的战略主动及其能力研究, 参与, 2024.1-2026.12 </li>
		<li> 2023, 国家级人工智能现代产业学院“产教融合型”教材（新编）揭榜挂帅项目, 虚拟现实技术, 主持, 2023.10-2025.8 </li>	
		<li> 2023, 横向研究课题, 虚拟现实图像目标检测与场景理解技术研究, 主持, 2023.6-2024.5 </li>	
		<li> 2022, 国家自然科学基金青年项目, 基于弱监督学习的高效视频显著性预测方法研究, 主持, 2023.1-2025.12</li>  
		<li> 2021, 中国博士后科学基金会面上项目, 遥感视频显著性预测关键技术研究, 主持, 2021.7-2023.2</li> 
		<li> 2021, 湖北省博士后创新研究岗位项目, 无人机视频显著性预测关键技术研究, 主持, 2021.7-2023.2 </li>	
		<li> 2021, 测绘遥感信息工程国家重点实验室探索类课题, 遥感视频显著性目标检测, 主持, 2021.1-2021.12 </li>	
		<li> 2018, 国家重点研发计划项目课题, 公共安全立体化协同监测关键技术, 参与, 2018.5-2021.4 </li>	
		<li> 2017, 国家重点研发计划项目课题, 融合多通道语境信息的类人智能感知机制与方法, 项目骨干, 2017.10-2021.9 </li>	
    </ul>
  </div>
</div>
</div>

<div style="clear: both;">
  <div class="section">
    <h2>Teaching</h2>
    <div class="paper">
      <ul>
      <li> Virtual Reality Technology, Digital Twin Technology, Digital Image Processing </li>	
      <li> Neural Network and Deep Learning, Introduction to Artificial Intelligence </li>
      </ul>
    </div>
  </div>
  </div>


<div style="clear: both;">
<div class="section">
  <h2>Services</h2>
  <div class="paper">
  	
    <ul>
		<li>2023.5-Now, Metaverse Technology and Application Innovation Platform of China, CIUR, Deputy secretary-general. </li>
		<li>2023, International Conference on Graphics and Image Processing (ICGIP), Publicity Co-chairs. </li>

    </ul>
	
  <p><b>Journal Reviewer</b>: </p>
    <ul>
		<li>IEEE Transactions on Image Processing (<b>TIP</b>)</li>
		<li>IEEE Transactions on Multimedia (<b>TMM</b>)</li>
    <li>IEEE Transactions on Circuits and Systems for Video Technology (<b>TCSVT</b>)</li>
		<li>IEEE Transactions on Geoscience and Remote Sensing  (<b>TGRS</b>)</li>
		<li>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing (<b>JSTARS</b>)</li>
		<li>IEEE Geoscience and Remote Sensing Letters (<b>GRSL</b>)</li>
	  <li>International Journal of Applied Earth Observation and Geoinformation (<b>JAG</b>)</li>
    </ul>
	 
	<p><b>Conference Reviewer</b>: </p>
    <ul>
		<li>IEEE International Conference on Image Processing (<b>ICIP</b>)</li>
		<li>IEEE International Conference on Multimedia and Expo (<b>ICME</b>)</li>
		<li>IEEE International Conference on Acoustics, Speech, and Signal Processing (<b>ICASSP</b>)</li>
    </ul>
  
</div>
</div>
</div>



<!--<div style="clear:both;">
<p align="right"><font size="5">Last Updated on 6th Sep, 2016</font></p> 
<p align="right"><font size="5"><a href="http://iip.whu.edu.cn/">I like this website </a></font></p>
</div>-->


<div class="jvectormap-tip"></div></body></html>
